{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53555c9d",
   "metadata": {},
   "source": [
    "# Decision tree for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d925a45",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579457f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load wine dataset from sklearn\n",
    "wine = load_wine()\n",
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b340a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################ Task 1 ############################################\n",
    "# Take attributes 'Alcohol', 'Malic acid' and 'Color intensity' as features for our further analysis\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "X = ...\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125b8b1",
   "metadata": {},
   "source": [
    "## Decision Tree Diagram\n",
    "<img src=\"dt.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffda8e0",
   "metadata": {},
   "source": [
    "## Decide What is a Good Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40194e05",
   "metadata": {},
   "source": [
    "#### Maximize Information Gain:\n",
    "\n",
    "$$Gain(split\\ point, feature) = Q(parent) - \\left(\\frac{N_{left}}{N}Q(left) + \\frac{N_{right}}{N}Q(right)\\right),$$\n",
    "\n",
    "where $Q$ is the impurity of a node. Common measures of impurity are Gini and Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9eb687",
   "metadata": {},
   "source": [
    "#### Impurity:\n",
    "\n",
    "Let the data at node $t$ be represented by $s_t$ with $n_t$ samples. If a target is a classification outcome taking on values $0,1,\\cdots,K-1$ for node $t$, let\n",
    "\n",
    "$$p_{tk} = \\frac{1}{n_t} \\sum_{y \\in s_t} 1(y=k)$$\n",
    "\n",
    "be the proportion of class $k$ at node $t$. Then Gini is defined by\n",
    "\n",
    "$$Q(s_t) = \\sum_k p_{tk}(1-p_{tk}),$$\n",
    "\n",
    "and Entropy is defined by\n",
    "\n",
    "$$Q(s_t) = -\\sum_k p_{tk}log(p_{tk}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c722d",
   "metadata": {},
   "source": [
    "## Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 2 ############################################\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "# Split the dataset into 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(...,...,test_size=...,random_state=0)\n",
    "\n",
    "# Setup a DecisionTreeClassifier with the Gini index as criterion\n",
    "dtc = ...\n",
    "\n",
    "# Fit DecisionTreeClassifier with training data\n",
    "...\n",
    "\n",
    "# Predict data in test set with DecisionTreeClassifier\n",
    "y_pred = ...\n",
    "\n",
    "# Calculate the accuracy of predictions\n",
    "accuracy = ...\n",
    "\n",
    "# Print the accuracy on the test set\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "# Display the learned classifier\n",
    "plt.figure(figsize=(15, 10))\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a8694",
   "metadata": {},
   "source": [
    "## Cost Complexity Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953fa40",
   "metadata": {},
   "source": [
    "The cost-complexity measure $R_{\\alpha}(T)$ of a given tree $T$ is:\n",
    "\n",
    "$$R_{\\alpha}(T) = R(T) + \\alpha |T|,$$\n",
    "\n",
    "where $|T|$ is the number of terminal nodes in $T$, $\\alpha \\geq 0$ is known as the complexity parameter, and $R(T)$ is defined as the total sample weighted impurity of the terminal nodes of $T$ in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e261e",
   "metadata": {},
   "source": [
    "Let $t$ be a node of the given tree $T$, and $T_t$ its branch, then the effective $\\alpha$ of a node is defined as\n",
    "\n",
    "$$\\alpha_{eff}(t) = \\frac{R(t) - R(T_t)}{|T| - 1}.$$\n",
    "\n",
    "A non-terminal node with the smallest value of $\\alpha_{eff}(t)$ is the weakest link and will be pruned. This process stops when the pruned tree’s minimal $\\alpha_{eff}(t)$ is greater than the ccp_alpha parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92915fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 3 ############################################\n",
    "# Use cost_complexity_pruning_path to display the resulting impurity in the pruned trees\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "# Use cost_complexity_pruning_path to return the effective alphas and the corresponding total leaf impurities\n",
    "path = dtc.cost_complexity_pruning_path(..., ...)\n",
    "effective_alphas, impurities = ..., ...\n",
    "\n",
    "# Plot the relationship of the effective alphas and the total leaf impurities\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(..., ..., marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 3 ############################################\n",
    "# Display the decision tree with the highest and second highest effective alpha value\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "# Setup a DecisionTreeClassifier with the highest effective alpha value\n",
    "dtc = DecisionTreeClassifier(criterion='gini', ccp_alpha=...)\n",
    "\n",
    "# Fit DecisionTreeClassifier with training data\n",
    "...\n",
    "# Plot the DecisionTreeClassifier\n",
    "plt.figure(figsize=(2, 1))\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 3 ############################################\n",
    "# Display the decision tree with the second highest effective alpha value\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "# Setup a DecisionTreeClassifier with the second highest effective alpha value\n",
    "dtc = DecisionTreeClassifier(criterion='gini', ccp_alpha=...)\n",
    "\n",
    "# Fit DecisionTreeClassifier with training data\n",
    "...\n",
    "\n",
    "# Plot the DecisionTreeClassifier\n",
    "plt.figure(figsize=(2, 1))\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 3 ############################################\n",
    "# Study the empirical and test error/risk of the pruned tree for α ∈ {0, 0.002, 0.004, 0.006, ... , 0.998, 0.1}.\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "# Setup decision tree with different values of ccp_alpha\n",
    "ccp_alphas = ...\n",
    "dtcs_alpha = [] # List of decision trees with different values of ccp_alpha\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    dtc_alpha = ...\n",
    "    ...\n",
    "    dtcs_alpha.append(...)\n",
    "\n",
    "# Compare the accuracy of training set and test set with different ccp_alpha values\n",
    "train_scores = [... for dtc in dtcs_alpha]\n",
    "test_scores = [... for dtc in dtcs_alpha]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 3 ############################################\n",
    "# Display the decision tree(s) with the highest test accuracy\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ccp_alphas = ...\n",
    "\n",
    "for i,alpha in enumerate(ccp_alphas):\n",
    "    \n",
    "    plt.subplot(1, len(ccp_alphas), i+1)\n",
    "    \n",
    "    # Setup a DecisionTreeClassifier with highest alpha value\n",
    "    dtc = ...\n",
    "\n",
    "    # Fit DecisionTreeClassifier with training data\n",
    "    ...\n",
    "\n",
    "    # Plot the DecisionTreeClassifier\n",
    "    ...\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848fb8dd",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ccd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46558017",
   "metadata": {},
   "source": [
    "max_features: The number of features to consider when looking for the best split.\n",
    "\n",
    "* If “sqrt”, then max_features=sqrt(n_features)\n",
    "\n",
    "* If “log2”, then max_features=log2(n_features)\n",
    "\n",
    "* If None, then max_features=n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 4 ############################################\n",
    "# Perform a grid search to find the best combination of criterion for impurity, \n",
    "# maximum number of features, and effective alpha value\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "# Construct a DecisionTreeClassifier\n",
    "dtc_gs = ...\n",
    "\n",
    "# Setup GridSearchCV\n",
    "params = ...\n",
    "\n",
    "gs = GridSearchCV(estimator=..., param_grid=..., scoring='accuracy', cv=5)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "...\n",
    "\n",
    "# Print the best combination of parameters\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 4 ############################################\n",
    "# Train a decision tree for the found best combination and estimate its generalization error\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "# Set up a DecisionTreeClassifier with the best parameters\n",
    "dtc_best = DecisionTreeClassifier(ccp_alpha=..., criterion=..., max_features=...)\n",
    "\n",
    "# Fit the DecisionTreeClassifier model\n",
    "...\n",
    "\n",
    "# Predict test data with the DecisionTreeClassifier model\n",
    "y_pred = ...\n",
    "\n",
    "# Calculate the accuracy of predictions\n",
    "accuracy = ...\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b270f0",
   "metadata": {},
   "source": [
    "## Add more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 5 ############################################\n",
    "# Increase the number of features starting with the chosen three features alcohol, malic acid,\n",
    "# and color intensity and adding the other available features one by one\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "# Build feature addition list\n",
    "feature_order = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bdacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 5 ############################################\n",
    "# Search each time the best combination of criterion for impurity, maximum number of features,\n",
    "# and effective alpha value. Report the resulting estimates for the generalization error versus the number of features.\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "scores_gs = []\n",
    "\n",
    "for i in range(3, len(feature_order) + 1):\n",
    "    \n",
    "    # Increase features gradually\n",
    "    selected = feature_order[:i]\n",
    "    X_cv = ...\n",
    "    y_cv = ...\n",
    "    \n",
    "    # Split the dataset into 80% train, 20% test\n",
    "    X_train, X_test, y_train, y_test = ...\n",
    "    \n",
    "    # Set up GridSearchCV\n",
    "    gs = GridSearchCV(estimator=..., param_grid=..., scoring='accuracy', cv=5, refit=True) \n",
    "   \n",
    "    # Fitting the model for grid search \n",
    "    ...\n",
    "\n",
    "    # Predict test data\n",
    "    y_pred_gs = ...\n",
    "    \n",
    "    # Calculate the accuracy of predictions\n",
    "    accuracy_gs = ...\n",
    "    \n",
    "    # Add the accuracy of GridSearchCV to a list\n",
    "    scores_gs.append(accuracy_gs)\n",
    "    \n",
    "# Plot cross validaton errors\n",
    "plt.title(\"DecisionTreeClassifier: Varying Number of Features\")\n",
    "plt.plot(np.arange(3,14), scores_gs, label=r\"$ \\epsilon_{gen} $ vs. the number of features\")\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d9fb8",
   "metadata": {},
   "source": [
    "## Features Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ce9c2",
   "metadata": {},
   "source": [
    "Let $f$ represent a feature and $t$ be a node of the given tree $T$, then\n",
    "\n",
    "$$Importance(f) = \\sum_{t\\in T} \\frac{N_t}{N} Gain_t(\\cdot,f),$$\n",
    "\n",
    "where $N_t/N$ is the proportion of samples reaching t, $Gain_t(\\cdot,f)$ is the information gain of nodes $t$ brought by feature $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 5 ############################################\n",
    "# Study the importance of all features\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "# Fit previous DecisionTreeClassifier with full dataset\n",
    "dtc_gs.fit(...,...)\n",
    "\n",
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=..., index=wine.feature_names)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f72a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 5 ############################################\n",
    "# Reorder the features according the their estimated importance\n",
    "\n",
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=...)\n",
    "\n",
    "# Sort importances\n",
    "index_list = importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Task 5 ############################################\n",
    "# Add now the features one by one, perform grid search, and report the estimates\n",
    "# for the generalization error versus the number of sorted features\n",
    "# ----------------------------------------- start here -----------------------------------------\n",
    "\n",
    "scores_gs = []\n",
    "selected = []\n",
    "\n",
    "for i in index_list.index:\n",
    "    \n",
    "    selected.append(i)\n",
    "    \n",
    "    # Increase features gradually\n",
    "    X_cv = ...\n",
    "    y_cv = ...\n",
    "    \n",
    "    # Split the dataset into 80% train, 20% test\n",
    "    X_train, X_test, y_train, y_test = ...\n",
    "    \n",
    "    # Set up GridSearchCV\n",
    "    gs = GridSearchCV(estimator=..., param_grid=..., scoring='accuracy', cv=5, refit=True) \n",
    "   \n",
    "    # Fitting the model for grid search \n",
    "    ...\n",
    "\n",
    "    # Predict test data\n",
    "    y_pred_gs = ...\n",
    "    \n",
    "    # Calculate the accuracy of predictions\n",
    "    accuracy_gs = ...\n",
    "\n",
    "    # Add the accuracy of GridSearchCV to a list\n",
    "    scores_gs.append(accuracy_gs)\n",
    "    \n",
    "# Plot cross validaton errors\n",
    "plt.title(\"DecisionTreeClassifier: Varying Number of Features\")\n",
    "plt.plot(np.arange(1,14), scores_gs, label=r\"$ \\epsilon_{gen} $ vs. the number of features\")\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962ced8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
