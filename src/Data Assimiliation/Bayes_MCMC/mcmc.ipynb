{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ef63c9-e114-45b5-890f-eefa41690e93",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b37daf-eab4-4637-b98c-ab02af018e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prior(h):\n",
    "    return norm.pdf(h, 1, 1)\n",
    "\n",
    "def likelihood(h, e):\n",
    "    return norm.pdf(e, h**2, 1)\n",
    "\n",
    "def posterior(h, e):\n",
    "    unnormalized = likelihood(h, e) * prior(h)\n",
    "    norm_const = np.trapz(unnormalized, h)\n",
    "    return unnormalized / norm_const\n",
    "\n",
    "h = np.linspace(-3, 4, 1000)\n",
    "e = 2\n",
    "\n",
    "f_prior = prior(h)\n",
    "f_likelihood = likelihood(h, e)\n",
    "f_post = posterior(h, e)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(h, f_prior, label=r'Prior $P(H)$', color='green')\n",
    "plt.plot(h, f_likelihood, label=r'Likelihood $P(E\\mid H)$', color='blue')\n",
    "plt.plot(h, f_post, label=r'Posterior $P(H\\mid E)$', color='black', linewidth=3)\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Prior, Likelihood, and Posterior Densities')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "max_post_h = h[f_post.argmax()]\n",
    "print(\"The posterior attains its maximum value at h = {:5.3f}\".format(max_post_h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b4392-b9e2-4b8b-9467-ad725a5c3292",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664893c-0c80-477d-9671-541c9e1275d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from matplotlib import cm\n",
    "\n",
    "A = np.array([[2], [1]])\n",
    "y = np.array([1, 2])\n",
    "S0 = 2\n",
    "m0 = 0\n",
    "\n",
    "gammas = [3.0, 2.0, 1.0, 0.5, 0.2]\n",
    "\n",
    "x_vals = np.linspace(-1.5, 2.0, 600)\n",
    "\n",
    "cmap = plt.get_cmap('gist_grey_r', len(gammas)+2)\n",
    "colors = [cmap(i+1) for i in range(len(gammas))]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "prior = norm.pdf(x_vals, loc=m0, scale=np.sqrt(S0))\n",
    "plt.plot(x_vals, prior, color='green', lw=1, linestyle='--', label=\"Prior\")\n",
    "\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    S0_inv = 1 / S0\n",
    "    Sigma_inv = np.eye(2) / (gamma ** 2)\n",
    "    At_Sinv_A = A.T @ Sigma_inv @ A\n",
    "    S_post = 1 / (S0_inv + At_Sinv_A[0, 0])\n",
    "    m_post = S_post * (S0_inv * m0 + (A.T @ Sigma_inv @ y)[0])\n",
    "    posterior = norm.pdf(x_vals, loc=m_post, scale=np.sqrt(S_post))\n",
    "    plt.plot(\n",
    "        x_vals, posterior, color=colors[idx], lw=2,\n",
    "        label=fr\"Posterior $\\gamma$={gamma}\"\n",
    "    )\n",
    "\n",
    "plt.axvline(0.8, color='black', linestyle='--', lw=2, label=\"True $x=4/5$\")\n",
    "plt.title(r\"Posterior distributions for different sensor noise $\\gamma$\")\n",
    "plt.xlabel(\"True temperature $x$\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378413c6-e7c9-40bb-9b21-4d5ccaa99e93",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988d4ef-46b8-4a9b-99cc-a64c464a375f",
   "metadata": {},
   "source": [
    "- Initialize the chain at some value for $\\mu$, say $\\mu_{\\text{current}}$.\n",
    "- Propose a new value $\\mu_{\\text{proposal}}$ by sampling from a normal distribution centered at $\\mu_{\\text{current}}$ with some proposal width.\n",
    "- Compute the acceptance probability:\n",
    "   $\n",
    "   p_{\\text{accept}} = \\min\\left(1, \\frac{P(x \\mid \\mu_{\\text{proposal}}) P(\\mu_{\\text{proposal}})}{P(x \\mid \\mu_{\\text{current}}) P(\\mu_{\\text{current}})}\\right)\n",
    "   $\n",
    "- Accept or reject the proposal:\n",
    "    - Accept the new value with probability $p_{accept}$.\n",
    "    - If accepted, set $\\mu_{\\text{current}} = \\mu_{\\text{proposal}}$; otherwise, stay at $\\mu_{\\text{current}}$.\n",
    "- Repeat steps 2–4 for many iterations to build up a chain of samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d271d3-d379-4d54-9904-d62b70c1a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "n = 20\n",
    "sigma = 15\n",
    "mu_prior = 130\n",
    "sigma_prior = 15\n",
    "\n",
    "true_mu = 125\n",
    "x = rng.normal(true_mu, sigma, size=n)\n",
    "\n",
    "sigma_post_sq = 1 / (1/sigma_prior**2 + n/sigma**2)\n",
    "mu_post = sigma_post_sq * (mu_prior/sigma_prior**2 + x.sum()/sigma**2)\n",
    "sigma_post = np.sqrt(sigma_post_sq)\n",
    "\n",
    "def log_posterior(mu, data, mu_prior, sigma_prior, sigma):\n",
    "    lp = norm.logpdf(mu, mu_prior, sigma_prior)\n",
    "    ll = np.sum(norm.logpdf(data, mu, sigma))\n",
    "    return lp + ll\n",
    "\n",
    "def metropolis_mcmc(log_posterior, data, mu_prior, sigma_prior, sigma, n_samples=10000, proposal_width=3.0, rng=None):\n",
    "    samples = np.empty(n_samples)\n",
    "    mu_current = np.mean(data)\n",
    "    logp_current = log_posterior(mu_current, data, mu_prior, sigma_prior, sigma)\n",
    "    n_accept = 0\n",
    "    for i in range(n_samples):\n",
    "        mu_proposal = rng.normal(mu_current, proposal_width)\n",
    "        logp_proposal = log_posterior(mu_proposal, data, mu_prior, sigma_prior, sigma)\n",
    "        log_accept_ratio = logp_proposal - logp_current\n",
    "        if np.log(rng.uniform()) < log_accept_ratio:\n",
    "            mu_current = mu_proposal\n",
    "            logp_current = logp_proposal\n",
    "            n_accept += 1\n",
    "        samples[i] = mu_current\n",
    "    acceptance_rate = n_accept / n_samples\n",
    "    return samples, acceptance_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85889b-652e-49e4-9084-c45b11e650cf",
   "metadata": {},
   "source": [
    "**Changing Proposal Width**\n",
    "\n",
    "- **Decreasing proposal_width (narrower proposals):**\n",
    "  - The sampler will propose new values very close to the current value.\n",
    "  - This leads to a high acceptance rate, but the chain moves slowly through the parameter space (poor *mixing*), potentially requiring many more samples to adequately explore the posterior\n",
    "  - The chain can become highly autocorrelated, reducing the effective sample size\n",
    "\n",
    "- **Increasing proposal_width (wider proposals):**\n",
    "  - The sampler proposes values farther from the current value.\n",
    "  - This can decrease the acceptance rate, as proposals often land in low-probability regions and are rejected.\n",
    "  - If too wide, the chain can get \"stuck\" for long periods, again reducing efficiency and potentially biasing results.\n",
    "\n",
    "- **Optimal proposal width:**\n",
    "  - There is a \"Goldilocks\" region where the proposal width is neither too small nor too large, leading to good mixing and an efficient exploration of the posterior.\n",
    "  - A common rule of thumb is to tune the proposal width so that the acceptance rate is around 20–50%, often aiming for about 0.234 in high dimensions or 40–50% in one dimension.\n",
    "\n",
    "**Changing Burn-in**\n",
    "\n",
    "- **Decreasing burn-in (shorter burn-in):**\n",
    "  - If the burn-in period is too short, samples may still be influenced by the arbitrary starting value and not yet representative of the true posterior.\n",
    "  - This can bias your estimates, especially if the chain takes a while to converge to the stationary distribution.\n",
    "\n",
    "- **Increasing burn-in (longer burn-in):**\n",
    "  - Discards more of the initial samples, ensuring that only samples from the stationary distribution are used for inference.\n",
    "  - However, setting burn-in much longer than necessary simply wastes samples and computational effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfa1bf-eea1-4914-9da1-370c2e7e99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_width = 3.0\n",
    "\n",
    "mcmc_samples, n_accept_ratio = metropolis_mcmc(\n",
    "    log_posterior,\n",
    "    x,\n",
    "    mu_prior,\n",
    "    sigma_prior,\n",
    "    sigma,\n",
    "    n_samples=2000,\n",
    "    proposal_width=proposal_width,\n",
    "    rng=rng\n",
    ")\n",
    "burn_in = 200\n",
    "mcmc_samples = mcmc_samples[burn_in:]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0,0].plot(mcmc_samples, alpha=0.7)\n",
    "axes[0,0].set_title(r\"MCMC Trace of $\\mu$\")\n",
    "axes[0,0].set_xlabel(\"Iteration\")\n",
    "axes[0,0].set_ylabel(r\"$\\mu$\")\n",
    "\n",
    "mu_grid = np.linspace(mu_post - 4*sigma_post, mu_post + 4*sigma_post, 500)\n",
    "axes[0,1].hist(mcmc_samples, bins=40, density=True, alpha=0.6, label=\"MCMC samples\")\n",
    "axes[0,1].plot(mu_grid, norm.pdf(mu_grid, mu_post, sigma_post), 'r-', lw=2, label=\"Analytical posterior\")\n",
    "axes[0,1].axvline(true_mu, color='k', ls='--', label=r\"True $\\mu$\")\n",
    "axes[0,1].set_title(r\"Posterior of $\\mu$\")\n",
    "axes[0,1].set_xlabel(r\"$\\mu$\")\n",
    "axes[0,1].set_ylabel(\"Density\")\n",
    "axes[0,1].legend()\n",
    "\n",
    "def autocorr(x, max_lag=50):\n",
    "    x = x - np.mean(x)\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    acorr = result[result.size // 2:]\n",
    "    acorr /= acorr[0]\n",
    "    return acorr[:max_lag+1]\n",
    "\n",
    "lags = np.arange(51)\n",
    "ac = autocorr(mcmc_samples, max_lag=50)\n",
    "axes[1,0].stem(lags, ac, basefmt=\" \")\n",
    "axes[1,0].set_title(\"Autocorrelation of MCMC samples\")\n",
    "axes[1,0].set_xlabel(\"Lag\")\n",
    "axes[1,0].set_ylabel(\"Autocorr\")\n",
    "\n",
    "axes[1,1].hist(x, bins=10, color='gray', alpha=0.7)\n",
    "axes[1,1].set_title(r\"20 Observed SBP measurements $\\mathcal{N}(125, 15^2)$\")\n",
    "axes[1,1].set_xlabel(\"SBP (mmHg)\")\n",
    "axes[1,1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Analytical posterior mean: {mu_post:.2f}, std: {sigma_post:.2f}\")\n",
    "print(f\"MCMC posterior mean: {np.mean(mcmc_samples):.2f}, std: {np.std(mcmc_samples):.2f}\")\n",
    "print(f\"True mean used for simulation: {true_mu:.2f}\")\n",
    "print(f\"Acceptance ratio: {100*n_accept_ratio:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d1823-190c-400e-9ac2-5f2099a4e824",
   "metadata": {},
   "source": [
    "A trace diagram (or trace plot) in MCMC: line plot showing the sampled values of a parameter (e.g., $\\mu$) over the course of the Markov chain iterations. \n",
    "\n",
    "- **Convergence**: check whether the chain has \"forgotten\" its starting value and settled into sampling from the target (posterior) distribution (fluctuating without long-term trends = good).\n",
    "- **Burn-in**: shows when the chain is moving from its starting value to the main region of interest (determine how many initial samples to remove).\n",
    "- **Mixing**: chain moving up and down rapidly = exploring the parameter space efficiently. Poor mixing is seen as the chain getting \"stuck\" or moving slowly, which suggests high autocorrelation and inefficiency.\n",
    "\n",
    "**Autocorrelation**: how each sample in the chain is correlated with previous samples at a given lag.\n",
    "\n",
    "**effective sample size (ESS)**: how many independent samples your correlated MCMC chain is equivalent to. Because MCMC samples are typically autocorrelated, the ESS is always less than or equal to the actual number of samples. \n",
    "\n",
    "| Step                     | Formula/Description                                                                 |\n",
    "|--------------------------|-------------------------------------------------------------------------------------|\n",
    "| Sample mean              | $$ \\hat{\\mu} = \\frac{1}{N} \\sum_{n=1}^N x_n $$                                     |\n",
    "| Autocovariance           | $$ \\hat{C}(t) = \\frac{1}{N-t} \\sum_{n=1}^{N-t} (x_{n+t} - \\hat{\\mu})(x_n - \\hat{\\mu}) $$ |\n",
    "| Autocorrelation          | $$ \\rho(t) = \\frac{\\hat{C}(t)}{\\hat{C}(0)} $$                                      |\n",
    "| Integrated autocorr. time| $$ \\tau_{\\text{int}} = 1 + 2 \\sum_{t=1}^{T} \\rho(t) $$                             |\n",
    "| ESS      | $$ \\frac{N}{1 + 2 \\sum_{t=1}^{\\infty} \\rho_t} $$ (sum is truncated at a lag where autocorrelations become negligible) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d15614-4177-4324-98da-a348a861ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr(x, max_lag=100):\n",
    "    x = x - np.mean(x)\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    acorr = result[result.size // 2:]\n",
    "    acorr /= acorr[0]\n",
    "    return acorr[:max_lag+1]\n",
    "\n",
    "def integrated_autocorr_time(acorr):\n",
    "    return 1 + 2 * np.sum(acorr[1:])\n",
    "\n",
    "def effective_sample_size(n_samples, iact):\n",
    "    return n_samples / iact\n",
    "\n",
    "max_lag = 100\n",
    "ac = autocorr(mcmc_samples, max_lag=max_lag)\n",
    "iact = integrated_autocorr_time(ac)\n",
    "ess = effective_sample_size(len(mcmc_samples), iact)\n",
    "\n",
    "print(f\"Integrated autocorrelation time (IACT): {iact:.2f}\")\n",
    "print(f\"Effective sample size (ESS): {ess:.1f} out of {len(mcmc_samples)} samples\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.stem(range(max_lag+1), ac, basefmt=\" \")\n",
    "ax.set_xlabel(\"Lag\")\n",
    "ax.set_ylabel(\"Autocorrelation\")\n",
    "ax.set_title(\"Autocorrelation Function of MCMC Samples\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c2328-712c-4d69-9595-0e097918e61c",
   "metadata": {},
   "source": [
    "- IACT: On average, it takes about 8.3 MCMC steps before the chain produces a sample that is nearly independent of the previous ones. The lower the IACT, the less correlated your samples are.\n",
    "- ESS: Although you have 18,000 MCMC samples, their autocorrelation means they are only as informative as about 2,168 independent samples. The higher the ESS relative to the total samples, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1e6f8-837e-40b0-9784-78054aa3df1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
