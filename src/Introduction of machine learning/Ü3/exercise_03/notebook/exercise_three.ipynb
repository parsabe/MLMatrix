{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01ec2a-c56a-47f7-9e82-9ba59201d800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from packaging import version\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "assert sys.version_info >= (3, 7)\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n",
    "\n",
    "# the next 5 lines define the default font sizes\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "def shuffle_and_split_data(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = shuffle_and_split_data(housing, 0.2)\n",
    "len(train_set)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "strat_splits = []\n",
    "for train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set_n = housing.iloc[train_index]\n",
    "    strat_test_set_n = housing.iloc[test_index]\n",
    "    strat_splits.append([strat_train_set_n, strat_test_set_n])\n",
    "\n",
    "strat_train_set, strat_test_set = strat_splits[0]\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)\n",
    "\n",
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall %\": income_cat_proportions(housing),\n",
    "    \"Stratified %\": income_cat_proportions(strat_test_set),\n",
    "    \"Random %\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props.index.name = \"Income Category\"\n",
    "compare_props[\"Strat. Error %\"] = (compare_props[\"Stratified %\"] /\n",
    "                                   compare_props[\"Overall %\"] - 1)\n",
    "compare_props[\"Rand. Error %\"] = (compare_props[\"Random %\"] /\n",
    "                                  compare_props[\"Overall %\"] - 1)\n",
    "(compare_props * 100).round(2)\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "\n",
    "housing = strat_train_set.copy()\n",
    "\n",
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15fe58-d506-42e5-a009-01ebdace8ad3",
   "metadata": {},
   "source": [
    "*End of Session two!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f627150-3a96-4f2f-9e4c-2185632dea36",
   "metadata": {},
   "source": [
    "# House Value ML\n",
    "*Your task is to predict median house values in Californian districts, given a number of features from these districts.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f407e-e186-4e9b-998b-a1387813a740",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c66cc-7ad2-4c69-b360-22fe0c9a4120",
   "metadata": {},
   "source": [
    "Let's revert to the original training set and separate the target (note that `strat_train_set.drop()` creates a copy of `strat_train_set` without the column, it doesn't actually modify `strat_train_set` itself, unless you pass `inplace=True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98890bc3-b6df-4d2f-a196-c086f1b37c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900296b2-9dc6-41f2-992c-5d607b817e08",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33762057-821e-4013-8b6b-97667a6d6226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_rows_idx = housing.isnull().any(axis=1)\n",
    "housing.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b72be-5c19-427e-bfbc-1fe4af600c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(housing.loc[null_rows_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782e0eb-40b7-49af-ad6a-49f28809f95c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question?\n",
    "And what do we do now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6069d1-250c-43c2-9dbd-71db858c0e37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean Up of Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610de90-bc63-4c5c-b57e-1e623a372c16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Option 1 - drop whole NaN entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e5083-b7db-412a-81bb-3e99204aaaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_option1 = housing.copy()\n",
    "\n",
    "housing_option1.dropna(subset=[\"total_bedrooms\"], inplace=True)  # option 1\n",
    "\n",
    "housing_option1.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3245e7-9120-4a9a-a30b-3f0d747effb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Option 2 - only drop NaN Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d6ac6-b437-4470-b42a-bf11ca3e557d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_option2 = housing.copy()\n",
    "\n",
    "housing_option2.drop(\"total_bedrooms\", axis=1, inplace=True)  # option 2\n",
    "\n",
    "housing_option2.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac6ebd-a6e4-47c7-aa1f-f59ca22c89da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Option 3 - fill the median for NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651550b4-78ee-41d4-bdda-4068c604b853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_option3 = housing.copy()\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing_option3[\"total_bedrooms\"].fillna(median, inplace=True)  # option 3\n",
    "\n",
    "housing_option3.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3576f94-77db-42eb-a5b9-96b883d33c9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Option 3 - the API way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6186501-4f89-4589-bce9-125c6f10b8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee841e-d452-4462-9d68-a88497dee006",
   "metadata": {},
   "source": [
    "Separating out the numerical attributes to use the `\"median\"` strategy (as it cannot be calculated on text attributes like `ocean_proximity`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fa6fd-9201-4cbc-87e1-e31d0ef5b071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec76114-b8f0-4648-b1d2-533228852bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c98db11-3dfc-41f5-b601-3b51eb567d3c",
   "metadata": {},
   "source": [
    "Check that this is the same as manually computing the median of each attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a7a48-ecd5-4649-bf2d-3886f6011c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41098778-47a0-43eb-9bb6-5c35669a15ae",
   "metadata": {},
   "source": [
    "Transform the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80292e-4f5a-4d95-970a-05b572fed708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)\n",
    "\n",
    "imputer.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e964bd93-caf0-45a0-9f3b-fa4b5c8ddd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca3e63-aa9f-4654-a852-36fcdeb8efe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)\n",
    "\n",
    "housing_tr.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffee88b-3b18-4922-8467-6129323cd6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d42f4-b2f9-47c2-a120-c54deba34800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputer.strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b010c-2e4c-44aa-b818-ff41e54ba5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51ab53-34d7-4dae-97ca-1faded8673c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")  # scikit-learn >= 1.2\n",
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e04835-7a17-4929-ab02-b26ba5bd6310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)\n",
    "\n",
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eef935-199c-48b3-bce0-42f0adf3555f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handling Text and Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053998c5-968e-490c-904b-31fee164eccd",
   "metadata": {},
   "source": [
    "Now let's preprocess the categorical input feature, `ocean_proximity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad0bee-c437-49eb-9bcb-4e47e47375f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9b473f-1039-4ee9-8568-0358073bdb66",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question?\n",
    "What's the Problem with this Attribute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ec42b-ea63-48c3-8c49-6b729118299e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4372f-8634-4563-a5fa-9d98a8e9b44f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_cat_encoded[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747084aa-c9c5-44c3-8bf8-958375b4fa42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74760ab7-392c-484b-bda1-ced0e5d042c4",
   "metadata": {},
   "source": [
    "### Question?\n",
    "\n",
    "Could this backfire?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab0855-1ba3-4b06-8a37-76837858b49d",
   "metadata": {},
   "source": [
    "### One-Hot\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c06cf-e5dc-4a93-8961-4446b075d960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder(sparse_output=False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a54ee-b320-483e-9c26-5a5bf26c8d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a283831-a257-4a6a-8374-1f2402ed220b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d462ad4-3754-4e71-90bb-0b54eed65636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]})\n",
    "pd.get_dummies(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dde156-fa03-433a-9fb8-7103c8d8c4be",
   "metadata": {},
   "source": [
    "The transformer \"remembers\"!  \n",
    "It got trained on data and this is stored!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881bdea-1133-4e66-80e9-9a91ecfb83ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_encoder.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5090a9d-60c1-490d-bc32-70bc559a56f2",
   "metadata": {},
   "source": [
    "### Unknown Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0ab91-b393-470a-a835-51c8671d1c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]})\n",
    "pd.get_dummies(df_test_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5828f8-19e7-4789-8bda-b7f5e6a9ccd5",
   "metadata": {},
   "source": [
    "**handle_unknown{‘error’, ‘ignore’, ‘infrequent_if_exist’}**, *default=’error’*  \n",
    "Specifies the way unknown categories are handled during transform.  \n",
    "\n",
    "*‘error’* : Raise an error if an unknown category is present during transform.  \n",
    "\n",
    "*‘ignore’* : When an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted as None.  \n",
    "\n",
    "*‘infrequent_if_exist’* : When an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will map to the infrequent category if it exists. The infrequent category will be mapped to the last position in the encoding. During inverse transform, an unknown category will be mapped to the category denoted 'infrequent' if it exists. If the 'infrequent' category does not exist, then transform and inverse_transform will handle an unknown category as with handle_unknown='ignore'. Infrequent categories exist based on min_frequency and max_categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02699e91-c49e-408a-9e5d-a9c77281fd89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_encoder.handle_unknown = \"ignore\"\n",
    "cat_encoder.transform(df_test_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04359c7d-eeb1-4036-bd09-8b97f94c0e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_encoder.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0188c45-6b2b-4eb1-8bef-8f66ac2cfcb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_encoder.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd32c5a-549f-4d22-b26c-c28ed1ad2d3e",
   "metadata": {},
   "source": [
    "In case we don't get a pandas dataframe we need to create one!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187d432-0804-4cee-85a7-1f73e1b36498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(cat_encoder.transform(df_test_unknown),\n",
    "                         columns=cat_encoder.get_feature_names_out(),\n",
    "                         index=df_test_unknown.index)\n",
    "\n",
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59eb7af-8f0b-489a-a6a7-cd8e8df01acd",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb659d-52e5-4c9a-9d30-d9b69a430401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fa132-2c62-44c3-8724-063c56cb0032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing[[\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d4838-d828-497e-9e46-562b13aedb7e",
   "metadata": {},
   "source": [
    "### Question?\n",
    "Look at the Numbers! Do you see any Problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cce2c-8fa3-4a0d-976a-cb55d1d6dcef",
   "metadata": {},
   "source": [
    "### Min Max Scaling\n",
    "$x_{new} = \\frac{x - min}{max-min}$  \n",
    "results in range 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f1035-d8c0-4d3a-ac91-63ce9840db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af70fb-1009-4700-925a-037a99a4eeec",
   "metadata": {},
   "source": [
    "### Standart Scaler\n",
    "$x_{new} = \\frac{x- \\text{mean}}{\\text{standart deviation}}$  \n",
    "\n",
    "reduces outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283bccc-71fc-41e3-9ca7-608f8e28a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0eb0be-13cf-47ab-84f5-ad998e476e9a",
   "metadata": {},
   "source": [
    "### Question?\n",
    "Okay, we can now scale ranges.  \n",
    "Will this work in every case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655925a-0611-4107-90b6-680801d2ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
    "housing[\"population\"].hist(ax=axs[0], bins=50)\n",
    "housing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\n",
    "axs[0].set_xlabel(\"Population\")\n",
    "axs[1].set_xlabel(\"Log of population\")\n",
    "axs[0].set_ylabel(\"Number of districts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9fcc16-7ad5-4c37-896e-1d888f2d2612",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bucketizing\n",
    "Heavy tailed distributions also can be bucketized (like income_median before)  \n",
    "Multi Modal distributions (different peaks) also can be bucketed, but same Problem as text labels (use one-hot).\n",
    "or a feature for each mode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d9f9f-b8dd-44ba-9147-18db12c060ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radial basis function to make it \"more gaussian\"\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbdec4-4c0d-45bf-8281-1b7ce9ae9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.linspace(housing[\"housing_median_age\"].min(),\n",
    "                   housing[\"housing_median_age\"].max(),\n",
    "                   500).reshape(-1, 1)\n",
    "gamma1 = 0.1\n",
    "gamma2 = 0.03\n",
    "rbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)\n",
    "rbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel(\"Housing median age\")\n",
    "ax1.set_ylabel(\"Number of districts\")\n",
    "ax1.hist(housing[\"housing_median_age\"], bins=50)\n",
    "\n",
    "ax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\n",
    "color = \"blue\"\n",
    "ax2.plot(ages, rbf1, color=color, label=\"gamma = 0.10\")\n",
    "ax2.plot(ages, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylabel(\"Age similarity\", color=color)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b3f98-fafe-43c7-8b96-2cfeb7dcfaa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Target Values\n",
    "Target Values also need to be transformed. If you replace a heavy tailed distribution with its log, then the log gets predicted!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d189f25-a7d8-4305-8458-28969ba0cf4f",
   "metadata": {},
   "source": [
    "### Solution\n",
    "inverse_transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444aac3-36b5-4c15-9d10-07b97a55d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(housing[[\"median_income\"]], scaled_labels)\n",
    "some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
    "\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "predictions = target_scaler.inverse_transform(scaled_predictions)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74283073-3efd-4bda-b04f-5563884f83c6",
   "metadata": {},
   "source": [
    "### Easier\n",
    "TransformedTargetRegressor and the transformer option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2e3f6-914e-45dd-bdcf-dbf80256df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "model = TransformedTargetRegressor(LinearRegression(),\n",
    "                                   transformer=StandardScaler())\n",
    "model.fit(housing[[\"median_income\"]], housing_labels)\n",
    "predictions = model.predict(some_new_data)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef437a7c-1222-43a1-a11a-7308d8c817e0",
   "metadata": {},
   "source": [
    "## Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c8258-094d-4526-b6fa-f768cd4de4b7",
   "metadata": {},
   "source": [
    "## Function Transformers\n",
    "Log transformer for the population feature. (right heavy tailed distributions can be easily switched with their log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c78923-ca94-4770-8b20-3d42a3bf4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\n",
    "log_pop = log_transformer.transform(housing[[\"population\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73626d-45a5-48ac-b62b-57c3493b468c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20522efc-b514-4436-b6d9-c865dcaeef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_transformer = FunctionTransformer(rbf_kernel,\n",
    "                                      kw_args=dict(Y=[[35.]], gamma=0.1))\n",
    "age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])\n",
    "\n",
    "age_simil_35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b25dc-d963-40f4-84e9-0fb365bc28d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Geographical Likeness with RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0bd77-97d3-4bfb-bb71-c8959c13f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_coords = 37.7749, -122.41\n",
    "sf_transformer = FunctionTransformer(rbf_kernel,\n",
    "                                     kw_args=dict(Y=[sf_coords], gamma=0.1))\n",
    "sf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])\n",
    "\n",
    "sf_simil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fb9a4-1624-4b56-ab0d-84b36a845b6e",
   "metadata": {},
   "source": [
    "## Trainable Transformers\n",
    "needs:\n",
    "- fit() for training\n",
    "- transform()\n",
    "- fit_transform() (TransformerMixin Class provides this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc5297-d47b-407f-a040-06c742176f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
    "        X = check_array(X)  # checks that X is an array with finite float values\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.scale_ = X.std(axis=0)\n",
    "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817e576-36d0-4294-8cda-862e7164ad9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clustering Example\n",
    "RBF Kernel to get the similarity of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7e6b2-03c4-4afc-a14a-bb7d8ed6bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n",
    "\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "similarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\n",
    "                                           sample_weight=housing_labels)\n",
    "\n",
    "similarities[:3].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f257f75-f434-496f-a629-1dbc1dc5408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_renamed = housing.rename(columns={\n",
    "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
    "    \"population\": \"Population\",\n",
    "    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\n",
    "housing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n",
    "\n",
    "housing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n",
    "                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n",
    "                     c=\"Max cluster similarity\",\n",
    "                     cmap=\"jet\", colorbar=True,\n",
    "                     legend=True, sharex=False, figsize=(10, 7))\n",
    "plt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n",
    "         cluster_simil.kmeans_.cluster_centers_[:, 0],\n",
    "         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n",
    "         label=\"Cluster centers\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93069455-7d01-4c6d-b94d-269b926935c0",
   "metadata": {},
   "source": [
    "## Transformation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652afbc-0289-4fd4-9550-0ebc7ff88c9c",
   "metadata": {},
   "source": [
    "Now let's build a pipeline to preprocess the numerical attributes:  \n",
    "Array of name Tansformer Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef05a6-6849-4517-a4d5-a31e04e937e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler()),\n",
    "])\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')\n",
    "\n",
    "num_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68627857-d898-44e0-bf64-3c11f160190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_prepared = num_pipeline.fit_transform(housing_num)\n",
    "housing_num_prepared[:2].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce182bb-7b7f-496d-9487-f898587cf8a6",
   "metadata": {},
   "source": [
    "### Patch some Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9f270-3602-4c38-a3ba-759679fb0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monkey_patch_get_signature_names_out():\n",
    "    \"\"\"Monkey patch some classes which did not handle get_feature_names_out()\n",
    "       correctly in Scikit-Learn 1.0.*.\"\"\"\n",
    "    from inspect import Signature, signature, Parameter\n",
    "    import pandas as pd\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.pipeline import make_pipeline, Pipeline\n",
    "    from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "    default_get_feature_names_out = StandardScaler.get_feature_names_out\n",
    "\n",
    "    if not hasattr(SimpleImputer, \"get_feature_names_out\"):\n",
    "      print(\"Monkey-patching SimpleImputer.get_feature_names_out()\")\n",
    "      SimpleImputer.get_feature_names_out = default_get_feature_names_out\n",
    "\n",
    "    if not hasattr(FunctionTransformer, \"get_feature_names_out\"):\n",
    "        print(\"Monkey-patching FunctionTransformer.get_feature_names_out()\")\n",
    "        orig_init = FunctionTransformer.__init__\n",
    "        orig_sig = signature(orig_init)\n",
    "\n",
    "        def __init__(*args, feature_names_out=None, **kwargs):\n",
    "            orig_sig.bind(*args, **kwargs)\n",
    "            orig_init(*args, **kwargs)\n",
    "            args[0].feature_names_out = feature_names_out\n",
    "\n",
    "        __init__.__signature__ = Signature(\n",
    "            list(signature(orig_init).parameters.values()) + [\n",
    "                Parameter(\"feature_names_out\", Parameter.KEYWORD_ONLY)])\n",
    "\n",
    "        def get_feature_names_out(self, names=None):\n",
    "            if callable(self.feature_names_out):\n",
    "                return self.feature_names_out(self, names)\n",
    "            assert self.feature_names_out == \"one-to-one\"\n",
    "            return default_get_feature_names_out(self, names)\n",
    "\n",
    "        FunctionTransformer.__init__ = __init__\n",
    "        FunctionTransformer.get_feature_names_out = get_feature_names_out\n",
    "\n",
    "monkey_patch_get_signature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3884b-aa12-44dd-8daa-9199419ef099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_num_prepared = pd.DataFrame(\n",
    "    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),\n",
    "    index=housing_num.index)\n",
    "\n",
    "df_housing_num_prepared.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145f41b-5081-43ee-b5dc-347fdb3383a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3699813-dfc8-4b0b-9e93-2eff155d414e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Indexing Supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d733f-19da-4510-843e-e1d2a1ebf415",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd483da-4245-41ff-8e65-a8fda010be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81adba7-5393-4d21-9673-4cd038651a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline.named_steps[\"simpleimputer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b78c66-d50b-4167-b1b5-6c88c38797e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline.set_params(simpleimputer__strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f43eb-e733-44b1-b8dc-cbde8e01e7ed",
   "metadata": {},
   "source": [
    "# Categorical and Numerical Values Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2840a-0076-4f7c-a229-d50c1d12f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) # sparse output is the default and this collides with pandas data frames\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1bbe71-4a51-4f36-8ed8-bba5ab5be24e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Automaticaly Selecting Attribute Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad7b7d-8459-416c-9e14-022261216a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
    ")\n",
    "\n",
    "housing_prepared = preprocessing.fit_transform(housing)\n",
    "housing_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4d948-dd2c-45bf-98f1-0f22aa4f40d5",
   "metadata": {},
   "source": [
    "## The Pipeline\n",
    "- missing numericals replaced by median\n",
    "- categoricals in one-hot encoding\n",
    "- some ratios computed and added\n",
    "- cluster similarity features added\n",
    "- long tails replaced by logarithm\n",
    "- numerical features normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9274fb2-fbf7-4432-9074-dada3a974c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_config(transform_output=\"default\")  # column_ratio bugs big time with dataframes\n",
    "\n",
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"]  # feature names out\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler())\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler())\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                                     StandardScaler())\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "                               \"households\", \"median_income\"]),\n",
    "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline)  # one column remaining: housing_median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bd973-e52f-497d-8827-4ec3560e952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = preprocessing.fit_transform(housing)\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4953f-6f69-4b66-9823-4f599d087440",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b449dc-7334-407a-af46-dd13d6347fbf",
   "metadata": {},
   "source": [
    "# Select and Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796f45c-d5b9-494a-9e5c-26b71eb676e9",
   "metadata": {},
   "source": [
    "## Training and Evaluating on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc179cc2-04ec-4520-9421-2b8531b7d5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
    "lin_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f69a0-5a7d-4756-90df-9b67bd6cc613",
   "metadata": {},
   "source": [
    "Let's try the full preprocessing pipeline on a few training instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d5ccb-6ca4-4396-81e2-c1ea61055353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_predictions = lin_reg.predict(housing)\n",
    "housing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53813588-2a26-4940-bcb2-608f31fbacea",
   "metadata": {},
   "source": [
    "Compare against the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2cfc1-3724-4cd6-8354-d74075918978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_labels.iloc[:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872880b-adca-4040-8f1d-3c9fa4cba26b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# computes the error ratios\n",
    "error_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1\n",
    "print(\", \".join([f\"{100 * ratio:.1f}%\" for ratio in error_ratios]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc22548-55d8-4213-95d4-e1b57ab90aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "lin_rmse = root_mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dbf6f2-e42e-4b5a-b23a-0ae854c69839",
   "metadata": {},
   "source": [
    "### Question\n",
    "What is happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1779dffb-e9d9-44fd-970c-bc9fa5bfbe63",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0b408-d0ad-4073-a57c-9b2a288f1b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
    "tree_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f27de-e2c3-4586-93da-29c232fe6e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing)\n",
    "tree_rmse = root_mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d2682-1d8a-4424-a55d-aa986555ce87",
   "metadata": {},
   "source": [
    "### Question\n",
    "What now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1c665-93f0-4496-a143-76d420c3b537",
   "metadata": {},
   "source": [
    "## Better Evaluation Using Cross-Validation\n",
    "with 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c66a35-c19e-45a9-9ddb-9d85eed7011e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a1b3c-2553-4d83-a2f0-89c290fe15e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.Series(tree_rmses).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d3831-2916-4039-b475-009f7db26770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# computes the error stats for the linear model\n",
    "lin_rmses = -cross_val_score(lin_reg, housing, housing_labels,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "pd.Series(lin_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544f5ff-b186-4d13-82eb-3bdfe7cdf266",
   "metadata": {},
   "source": [
    "## RandomForest\n",
    "**Warning:** the following cell may take a few minutes to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579f886-a5f7-4f6d-9d01-c93d7bb423ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = make_pipeline(preprocessing,\n",
    "                           RandomForestRegressor(random_state=42))\n",
    "forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\n",
    "                                scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba928f-0797-4c26-ba0e-d02d6b87a802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.Series(forest_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b59337-081f-4b00-9bb8-c2c0b23713b0",
   "metadata": {},
   "source": [
    "Let's compare this RMSE measured using cross-validation (the \"validation error\") with the RMSE measured on the training set (the \"training error\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e8dce-65dd-4eee-a196-18d885ae99a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest_reg.fit(housing, housing_labels)\n",
    "housing_predictions = forest_reg.predict(housing)\n",
    "forest_rmse = root_mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d83c0-5e3c-4b49-a47e-26e14c58a052",
   "metadata": {},
   "source": [
    "The training error is much lower than the validation error, which usually means that the model has overfit the training set. Another possible explanation may be that there's a mismatch between the training data and the validation data, but it's not the case here, since both came from the same dataset that we shuffled and split in two parts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
